# Experiment: Gendergerechte Embeddings

![](/image.png)

## KI-gerechtes Gendern von Begriffen?

Wenn #Google bald keine Suchmaschine ist, sondern ein #KI-#Chatbot, Ã¤ndert das nicht nur die SEO-Strategie, sondern auch unsere Sprache. 

Nehmen wir die Idee der gendergerechten Sprache - neben der Gerechtigkeit mÃ¶chte ich auch, dass unsere Stellenanzeigen bei der @viadee weiterhin von Google gefunden werden. Wie schreibe ich dann am besten "Gesucht: Senior IT-Beraterin Data & AI"?

Ich habe mich daher zu einem kleinen Experiment entschlossen. Wie Ã¤hnlich ist aus KI-Sicht der "IT-Berater" zur "IT-Beraterin", "IT-Berater:in", "IT-Berater*in"? ðŸ¤” Es mÃ¼sste eine rechnerisch optimale Formulierung geben, und zwar die, in der sich die feminine und die maskuline Formulierung am wenigsten voneinander unterscheiden, denn wir mÃ¶chten mÃ¶glichst von allen qualifizierten Menschen gefunden werden.

Das lÃ¤sst sich mit dem Google Embedding-Modell (`gemini-embedding-001`), ein wenig Python-Code und der Ã¼blichen Cosinus-Ã„hnlichkeit sehr einfach messen, bspw. in einer @qdrant-Vektordatenbank.

Die semantischen Distanzen sind im Ergebnis wie folgt:

â€¢ IT-Berater ðŸ†š IT-Berater:in: 0.0386

â€¢ IT-Berater ðŸ†š IT-Berater*in: 0.0392

â€¢ IT-Berater ðŸ†š IT-Beratende: 0.0482

â€¢ IT-Berater ðŸ†š IT-Beraterin: 0.0503

Danach kommen mit einigem Abstand meine stereotypen Kontrollbegriffe "Bademeisterin" und "Hufschmiedin" und ich stelle fest, dass mein Beruf dem eines Bademeisters Ã¤hnlicher ist als dem eines Hufschmieds. Nun gut.

Zwischenfazit: Die *IT-Berater:in* wÃ¤re also die beste Wahl, um mÃ¶glichst viele Menschen in Suchanfragen diskriminierungsfrei zu erreichen. FÃ¼r die beliebten Modelle von OpenAI (hier `text-embedding-3-small`) gilt das Ã¼brigens auch.

Allerdings ist der Doppelpunkt auch ein Ã¼bliches Trennzeichen und kein Wortbestandteil. FÃ¼r die KI-Sicht der Stellenanzeige ist "Gesucht: IT-Berater in (.. KÃ¶ln)" auch eine mÃ¶gliche Fortsetzung des Textes. Dann hÃ¤tten wir hier nur zwei MÃ¤nnerrollen verglichen, und es fehlt nur zufÃ¤llig die Stadt. 

Vielleicht funktioniert der Begriff zwar am besten, aber nur aus den 'falschen' GrÃ¼nden und nicht, weil das Google-Modell angemessen viele deutsche Texte in gendergerechter Sprache als Trainingsvorlage hatte? Ich wÃ¼rde trotzdem pragmatisch zu diesem Begriff raten und wir verwenden ihn glÃ¼cklicherweise auch schon in unseren Stellenanzeigen... oder seine englischsprachigen Varianten.

Diese Begriffe lesen sich auch gut. "IT-Consultant" ist im Vector-Space aber semantisch doppelt so weit entfernt vom IT-Berater wie die IT-Beraterin. Das Experiment lÃ¤sst mich in jedem Fall mit dem Wunsch nach mehr europÃ¤ischer Sprachkompetenz in den KI-Modellen zurÃ¼ck.

Der Link zum Git-Repository mit dem Experiment folgt in den Kommentaren.

â–¶ï¸ https://github.com/fkoehne/gendergerechte-embeddings

## Experiment nachvollziehen

Wir nehmen eine Python-Entwicklungsumgebung sowie deren Konfiguration mit OpenAI und Google Vertex an.

Als konkretes Setup bleibt dann nur noch der Start der Qdrant-DB Ã¼brig. 

Danach kÃ¶nnen die beiden Notebooks in diesem Repository mit [OpenAI-Embeddings](gendergerecht.ipynb) und [Google Gemini-Embeddings](gendergerecht-gemini.ipynb) ausgefÃ¼hrt werden, um das Experiment nachzuvollziehen.

```bash
podman run -p 6333:6333 qdrant/qdrant
```
# Bild-Quelle

KI-generiert per `Stable Diffusion 3.5 Ultra`
> A friendly robot (orange) putting one finger to her/his face, thinking about which of two doors to open. The left door is a WC marked with a male shape, the right one with a female shape.